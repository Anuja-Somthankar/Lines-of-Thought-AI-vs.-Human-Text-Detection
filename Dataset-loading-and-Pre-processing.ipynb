{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "PVrLdZXHAhL0",
    "outputId": "92eddce6-434d-4577-c2fc-8885e2e76ee7"
   },
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!pip install datasets\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install transformers==3.2.0\n",
    "!pip install transformers[sentencepiece]\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ORm6IjReEHHj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anuja\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import concurrent.futures\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "from transformers import DistilBertTokenizer, AlbertTokenizer, BertTokenizer, ElectraTokenizer\n",
    "from transformers import AlbertForSequenceClassification, DistilBertForSequenceClassification, BertForSequenceClassification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uoEpUMMjEo9T",
    "outputId": "3409adb0-b771-44dd-ccfe-8f34ca159d05"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anuja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\anuja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seed_everything(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q6_OBzHHAQ4R",
    "outputId": "cfa72209-4e6d-4aeb-9481-ea3bfe1769b8"
   },
   "outputs": [],
   "source": [
    "ds = load_dataset(\"Jinyan1/COLING_2025_MGT_en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yq_fSYBBAyvN",
    "outputId": "d1601ad2-b30c-43f7-f2ba-ff262ac596a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'source', 'sub_source', 'lang', 'model', 'label', 'text'],\n",
       "        num_rows: 610767\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['id', 'source', 'sub_source', 'lang', 'model', 'label', 'text'],\n",
       "        num_rows: 261758\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "4vj36ZORBt_G"
   },
   "outputs": [],
   "source": [
    "training_dataset = ds[\"train\"]\n",
    "train_dataset = training_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "02cXISTdCERP",
    "outputId": "8060264e-9d06-4f13-aa57-e2560746e503"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hitler plan succession power structure death s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bush administration turned attention iraq argu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>best pedi pho attentive rush time come vega co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>m mv loch seaforth passenger sailing month tes...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>malta participated eurovision song contest son...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  hitler plan succession power structure death s...      1\n",
       "1  bush administration turned attention iraq argu...      0\n",
       "2  best pedi pho attentive rush time come vega co...      0\n",
       "3  m mv loch seaforth passenger sailing month tes...      0\n",
       "4  malta participated eurovision song contest son...      0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataset:  610767\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset[['text', 'label']]\n",
    "print(\"Length of the dataset: \", len(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"Jinyan1/COLING_2025_MGT_en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'source', 'sub_source', 'lang', 'model', 'label', 'text'],\n",
       "        num_rows: 610767\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['id', 'source', 'sub_source', 'lang', 'model', 'label', 'text'],\n",
       "        num_rows: 261758\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "development_dataset = ds[\"dev\"]\n",
    "dev_dataset = development_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>sub_source</th>\n",
       "      <th>lang</th>\n",
       "      <th>model</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e0c8d183-c377-4af0-a12c-2204d75cd5f0</td>\n",
       "      <td>m4gt</td>\n",
       "      <td>peerread</td>\n",
       "      <td>en</td>\n",
       "      <td>gpt4</td>\n",
       "      <td>1</td>\n",
       "      <td>The paper titled \"A Transition-Based Directed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2b6d18d5-993f-486f-a631-986f46ec7ba0</td>\n",
       "      <td>mage</td>\n",
       "      <td>wp</td>\n",
       "      <td>en</td>\n",
       "      <td>text-davinci-003</td>\n",
       "      <td>1</td>\n",
       "      <td>(Apologies for two submissions, but need to wr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>327667aa-fbe2-46dc-b2e9-31c7618845ca</td>\n",
       "      <td>mage</td>\n",
       "      <td>cmv</td>\n",
       "      <td>en</td>\n",
       "      <td>7B</td>\n",
       "      <td>1</td>\n",
       "      <td>WARNING: WALL OF TEXT!!! I also jump from topi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2fa81a5f-a6da-4cd5-ab0b-22ea4e0464b9</td>\n",
       "      <td>m4gt</td>\n",
       "      <td>outfox</td>\n",
       "      <td>en</td>\n",
       "      <td>cohere</td>\n",
       "      <td>1</td>\n",
       "      <td>Emotion recognition through facial feedback ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61b935f0-3a00-4441-a185-34216bc5b55a</td>\n",
       "      <td>mage</td>\n",
       "      <td>eli5</td>\n",
       "      <td>en</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>1</td>\n",
       "      <td>Several things. 1. The cooling effect of air c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id source sub_source lang  \\\n",
       "0  e0c8d183-c377-4af0-a12c-2204d75cd5f0   m4gt   peerread   en   \n",
       "1  2b6d18d5-993f-486f-a631-986f46ec7ba0   mage         wp   en   \n",
       "2  327667aa-fbe2-46dc-b2e9-31c7618845ca   mage        cmv   en   \n",
       "3  2fa81a5f-a6da-4cd5-ab0b-22ea4e0464b9   m4gt     outfox   en   \n",
       "4  61b935f0-3a00-4441-a185-34216bc5b55a   mage       eli5   en   \n",
       "\n",
       "              model  label                                               text  \n",
       "0              gpt4      1  The paper titled \"A Transition-Based Directed ...  \n",
       "1  text-davinci-003      1  (Apologies for two submissions, but need to wr...  \n",
       "2                7B      1  WARNING: WALL OF TEXT!!! I also jump from topi...  \n",
       "3            cohere      1  Emotion recognition through facial feedback ha...  \n",
       "4     gpt-3.5-turbo      1  Several things. 1. The cooling effect of air c...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataset:  261758\n"
     ]
    }
   ],
   "source": [
    "dev_dataset = dev_dataset[['text', 'label']]\n",
    "print(\"Length of the dataset: \", len(dev_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The paper titled \"A Transition-Based Directed ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Apologies for two submissions, but need to wr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WARNING: WALL OF TEXT!!! I also jump from topi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Emotion recognition through facial feedback ha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Several things. 1. The cooling effect of air c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  The paper titled \"A Transition-Based Directed ...      1\n",
       "1  (Apologies for two submissions, but need to wr...      1\n",
       "2  WARNING: WALL OF TEXT!!! I also jump from topi...      1\n",
       "3  Emotion recognition through facial feedback ha...      1\n",
       "4  Several things. 1. The cooling effect of air c...      1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_json('test_set_en_with_label.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>model</th>\n",
       "      <th>prompt_flag</th>\n",
       "      <th>prompt</th>\n",
       "      <th>domain</th>\n",
       "      <th>paper_id</th>\n",
       "      <th>binary</th>\n",
       "      <th>mixset_category</th>\n",
       "      <th>testset_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello, Thanks for sharing your health concern ...</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "      <td>CUDRT</td>\n",
       "      <td>human</td>\n",
       "      <td>NA</td>\n",
       "      <td>hello Dr. ! I am married since 2 years and my ...</td>\n",
       "      <td>GPT3.5_QA</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In primary school, especially in the countrysi...</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "      <td>ieltsduck</td>\n",
       "      <td>human</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>ielts</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The advent of artificial intelligence (AI) has...</td>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "      <td>ieltsduck</td>\n",
       "      <td>gpt-4o-mini-2024-07-18</td>\n",
       "      <td>improved</td>\n",
       "      <td>Please act as a student preparing for the IELT...</td>\n",
       "      <td>ielts</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Unemployment insurance through options refers ...</td>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "      <td>CUDRT</td>\n",
       "      <td>GPT3.5</td>\n",
       "      <td>NA</td>\n",
       "      <td>Unemployment Insurance Through Options</td>\n",
       "      <td>GPT3.5_QA</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The long exposure feature on DSLR cameras work...</td>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "      <td>CUDRT</td>\n",
       "      <td>ChatGLM</td>\n",
       "      <td>NA</td>\n",
       "      <td>How the long exposure feature on DSLR Cameras ...</td>\n",
       "      <td>ChatGLM_QA</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text language  label  \\\n",
       "0  Hello, Thanks for sharing your health concern ...  English      0   \n",
       "1  In primary school, especially in the countrysi...  English      0   \n",
       "2  The advent of artificial intelligence (AI) has...  English      1   \n",
       "3  Unemployment insurance through options refers ...  English      1   \n",
       "4  The long exposure feature on DSLR cameras work...  English      1   \n",
       "\n",
       "      source                   model prompt_flag  \\\n",
       "0      CUDRT                   human          NA   \n",
       "1  ieltsduck                   human          NA   \n",
       "2  ieltsduck  gpt-4o-mini-2024-07-18    improved   \n",
       "3      CUDRT                  GPT3.5          NA   \n",
       "4      CUDRT                 ChatGLM          NA   \n",
       "\n",
       "                                              prompt      domain paper_id  \\\n",
       "0  hello Dr. ! I am married since 2 years and my ...   GPT3.5_QA     None   \n",
       "1                                                 NA       ielts     None   \n",
       "2  Please act as a student preparing for the IELT...       ielts     None   \n",
       "3             Unemployment Insurance Through Options   GPT3.5_QA     None   \n",
       "4  How the long exposure feature on DSLR Cameras ...  ChatGLM_QA     None   \n",
       "\n",
       "  binary mixset_category  testset_id  \n",
       "0   None            None           0  \n",
       "1   None            None           1  \n",
       "2   None            None           2  \n",
       "3   None            None           3  \n",
       "4   None            None           4  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataset:  73941\n"
     ]
    }
   ],
   "source": [
    "test_dataset = ds[['text', 'label']]\n",
    "print(\"Length of the dataset: \", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello, Thanks for sharing your health concern ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In primary school, especially in the countrysi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The advent of artificial intelligence (AI) has...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Unemployment insurance through options refers ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The long exposure feature on DSLR cameras work...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Hello, Thanks for sharing your health concern ...      0\n",
       "1  In primary school, especially in the countrysi...      0\n",
       "2  The advent of artificial intelligence (AI) has...      1\n",
       "3  Unemployment insurance through options refers ...      1\n",
       "4  The long exposure feature on DSLR cameras work...      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Dataset Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_batch(batch_texts):\n",
    "    processed_batch = []\n",
    "    for text in batch_texts:\n",
    "        text = text.lower()\n",
    "        doc = nlp(text)\n",
    "        tokens = []\n",
    "        for token in doc:\n",
    "            if token.is_alpha and not token.is_stop:\n",
    "                tokens.append(lemmatizer.lemmatize(token.text))\n",
    "        processed_batch.append(' '.join(tokens))\n",
    "    return processed_batch\n",
    "\n",
    "def preprocess_text_parallel(dataset):\n",
    "    batch_size = 1000\n",
    "    n = len(dataset)\n",
    "    processed_texts = []\n",
    "    batches = []\n",
    "    for i in range(0, n, batch_size):\n",
    "        batch = dataset['text'][i:i+batch_size].tolist()\n",
    "        batches.append(batch)\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = executor.map(process_text_batch, batches)\n",
    "        \n",
    "    for batch in results:\n",
    "        processed_texts.extend(batch)\n",
    "    \n",
    "    dataset['text'] = processed_texts\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataset = preprocess_text_parallel(train_dataset) # 175m 4.8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "True    610767\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_dataset['label'] == cleaned_dataset['label']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataset.to_csv('cleaned_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development Dataset Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_batch(batch_texts):\n",
    "    processed_batch = []\n",
    "    for text in batch_texts:\n",
    "        text = text.lower()\n",
    "        doc = nlp(text)\n",
    "        tokens = []\n",
    "        for token in doc:\n",
    "            if token.is_alpha and not token.is_stop:\n",
    "                tokens.append(lemmatizer.lemmatize(token.text))\n",
    "        processed_batch.append(' '.join(tokens))\n",
    "    return processed_batch\n",
    "\n",
    "def preprocess_text_parallel(dataset):\n",
    "    batch_size = 1000\n",
    "    n = len(dataset)\n",
    "    processed_texts = []\n",
    "    batches = []\n",
    "    for i in range(0, n, batch_size):\n",
    "        batch = dataset['text'][i:i+batch_size].tolist()\n",
    "        batches.append(batch)\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = executor.map(process_text_batch, batches)\n",
    "        \n",
    "    for batch in results:\n",
    "        processed_texts.extend(batch)\n",
    "    \n",
    "    dataset['text'] = processed_texts\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dev_dataset = preprocess_text_parallel(dev_dataset) #127m 14.6s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "True    261758\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dev_dataset['label'] == cleaned_dev_dataset['label']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dev_dataset.to_csv('cleaned_dev_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>paper titled transition based directed aciclic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apology submission need write dear powerfalcon...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>warning wall text jump topic topic transition ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>emotion recognition facial feedback subject gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thing cooling effect air current taking away i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  paper titled transition based directed aciclic...      1\n",
       "1  apology submission need write dear powerfalcon...      1\n",
       "2  warning wall text jump topic topic transition ...      1\n",
       "3  emotion recognition facial feedback subject gr...      1\n",
       "4  thing cooling effect air current taking away i...      1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_dev_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Dataset Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_batch(batch_texts):\n",
    "    processed_batch = []\n",
    "    for text in batch_texts:\n",
    "        text = text.lower()\n",
    "        doc = nlp(text)\n",
    "        tokens = []\n",
    "        for token in doc:\n",
    "            if token.is_alpha and not token.is_stop:\n",
    "                tokens.append(lemmatizer.lemmatize(token.text))\n",
    "        processed_batch.append(' '.join(tokens))\n",
    "    return processed_batch\n",
    "\n",
    "def preprocess_text_parallel(dataset):\n",
    "    batch_size = 1000\n",
    "    n = len(dataset)\n",
    "    processed_texts = []\n",
    "    batches = []\n",
    "    for i in range(0, n, batch_size):\n",
    "        batch = dataset['text'][i:i+batch_size].tolist()\n",
    "        batches.append(batch)\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = executor.map(process_text_batch, batches)\n",
    "        \n",
    "    for batch in results:\n",
    "        processed_texts.extend(batch)\n",
    "    \n",
    "    dataset['text'] = processed_texts\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_test_dataset = preprocess_text_parallel(test_dataset) #53m 11.3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "True    73941\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_dataset['label'] == cleaned_test_dataset['label']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_test_dataset.to_csv('cleaned_test_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hello thanks sharing health concern gone query...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>primary school especially countryside japan pr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>advent artificial intelligence ai sparked vigo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unemployment insurance option refers use finan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>long exposure feature dslr camera work allowin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  hello thanks sharing health concern gone query...      0\n",
       "1  primary school especially countryside japan pr...      0\n",
       "2  advent artificial intelligence ai sparked vigo...      1\n",
       "3  unemployment insurance option refers use finan...      1\n",
       "4  long exposure feature dslr camera work allowin...      1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_test_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization of Training Dataset for the BERT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataset = pd.read_csv('cleaned_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anuja\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\anuja\\.cache\\huggingface\\hub\\models--google--electra-small-discriminator. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "tinybert_tokenizer = BertTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "electra_small_tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_texts = [str(text) for text in cleaned_dataset['text'].values if text is not None]\n",
    "DB_inputs = distilbert_tokenizer(cleaned_texts, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "TB_inputs = tinybert_tokenizer(cleaned_texts, padding=True, truncation=True, max_length=512, return_tensors='pt') \n",
    "EL_inputs = electra_small_tokenizer(cleaned_texts, padding=True, truncation=True, max_length=512, return_tensors='pt') # 1 hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor(cleaned_dataset['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([610767, 512])\n",
      "torch.Size([610767, 512])\n",
      "torch.Size([610767, 512])\n",
      "torch.Size([610767])\n"
     ]
    }
   ],
   "source": [
    "print(DB_inputs['input_ids'].shape)\n",
    "print(TB_inputs['input_ids'].shape)\n",
    "print(EL_inputs['input_ids'].shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(DB_inputs, 'intermediates/DB_inputs.pt')\n",
    "torch.save(TB_inputs, 'intermediates/TB_inputs.pt')\n",
    "torch.save(EL_inputs, 'intermediates/EL_inputs.pt')\n",
    "torch.save(labels, 'intermediates/labels.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization of Development Dataset for the BERT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dev_dataset = pd.read_csv('cleaned_dev_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "tinybert_tokenizer = BertTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "electra_small_tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dev_texts = [str(text) for text in cleaned_dev_dataset['text'].values if text is not None]\n",
    "DB_dev_inputs = distilbert_tokenizer(cleaned_dev_texts, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "EL_dev_inputs = electra_small_tokenizer(cleaned_dev_texts, padding=True, truncation=True, max_length=512, return_tensors='pt') #\n",
    "TB_dev_inputs = tinybert_tokenizer(cleaned_dev_texts, padding=True, truncation=True, max_length=512, return_tensors='pt') #32m 4.7s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dev = torch.tensor(cleaned_dev_dataset['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([261758, 512])\n",
      "torch.Size([261758, 512])\n",
      "torch.Size([261758, 512])\n",
      "torch.Size([261758])\n"
     ]
    }
   ],
   "source": [
    "print(DB_dev_inputs['input_ids'].shape)\n",
    "print(EL_dev_inputs['input_ids'].shape)\n",
    "print(TB_dev_inputs['input_ids'].shape)\n",
    "print(labels_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(DB_dev_inputs, 'intermediates_dev/DB_dev_inputs.pt')\n",
    "torch.save(EL_dev_inputs, 'intermediates/EL_dev_inputs.pt')\n",
    "torch.save(TB_dev_inputs, 'intermediates_dev/TB_dev_inputs.pt')\n",
    "torch.save(labels_dev, 'intermediates_dev/labels_dev.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization of Test Dataset for the BERT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_test_dataset = pd.read_csv('cleaned_test_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "tinybert_tokenizer = BertTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
    "electra_small_tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_test_texts = [str(text) for text in cleaned_test_dataset['text'].values if text is not None]\n",
    "DB_test_inputs = distilbert_tokenizer(cleaned_test_texts, padding=True, truncation=True, max_length=512, return_tensors='pt') \n",
    "EL_test_inputs = electra_small_tokenizer(cleaned_test_texts, padding=True, truncation=True, max_length=512, return_tensors='pt') #\n",
    "TB_test_inputs = tinybert_tokenizer(cleaned_test_texts, padding=True, truncation=True, max_length=512, return_tensors='pt') #10m 23.7s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test = torch.tensor(cleaned_test_dataset['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([73941, 512])\n",
      "torch.Size([73941, 512])\n",
      "torch.Size([73941, 512])\n",
      "torch.Size([73941])\n"
     ]
    }
   ],
   "source": [
    "print(DB_test_inputs['input_ids'].shape)\n",
    "print(EL_test_inputs['input_ids'].shape)\n",
    "print(TB_test_inputs['input_ids'].shape)\n",
    "print(labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(DB_test_inputs, 'intermediates/DB_test_inputs.pt')\n",
    "torch.save(EL_test_inputs, 'intermediates/EL_test_inputs.pt')\n",
    "torch.save(TB_test_inputs, 'intermediates/TB_test_inputs.pt')\n",
    "torch.save(labels_test, 'intermediates/labels_test.pt')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
