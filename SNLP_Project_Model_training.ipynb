{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vishal Perumal\\anaconda3\\envs\\gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import concurrent.futures\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AlbertForSequenceClassification, DistilBertForSequenceClassification, BertForSequenceClassification\n",
    "from transformers import DistilBertModel, AdamW\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_DB_inputs = torch.load('intermediates/DB_inputs.pt')\n",
    "dev_DB_inputs = torch.load('intermediates/DB_dev_inputs.pt')\n",
    "\n",
    "train_labels = torch.load('intermediates/labels.pt')\n",
    "dev_labels = torch.load('intermediates/labels_dev.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dB_inputs, labels):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        :param dB_inputs: Dictionary containing the tokenized inputs from the tokenizer (e.g., input_ids, attention_mask).\n",
    "        :param labels: List or tensor containing the corresponding labels for each example.\n",
    "        \"\"\"\n",
    "        self.dB_inputs = dB_inputs  # The tokenized inputs (input_ids, attention_mask, etc.)\n",
    "        self.labels = labels  # The target labels (e.g., 0 or 1 for binary classification)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of samples\n",
    "        return len(self.dB_inputs['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the sample corresponding to index `idx`\n",
    "        input_ids = self.dB_inputs['input_ids'][idx]\n",
    "        attention_mask = self.dB_inputs['attention_mask'][idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': label\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_custom_dataset = CustomDataset(dB_inputs=train_DB_inputs, labels=train_labels)\n",
    "train_custom_dataset = CustomDataset(dB_inputs=dev_DB_inputs, labels=dev_labels)\n",
    "\n",
    "train_dataloader = DataLoader(train_custom_dataset, batch_size=64, shuffle=True)\n",
    "dev_dataloader = DataLoader(train_custom_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Pretrained and Finetuning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv('cleaned_dataset.csv')\n",
    "dev_dataset = pd.read_csv('cleaned_dev_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLSTMClassifier(nn.Module):\n",
    "    def __init__(self, distilbert, cnn_out_channels=64, lstm_hidden_dim=64, num_classes=2):\n",
    "        super(CNNLSTMClassifier, self).__init__()\n",
    "        self.distilbert = distilbert\n",
    "        self.cnn = nn.Conv1d(in_channels=768, out_channels=cnn_out_channels, kernel_size=3, padding=1)\n",
    "        self.lstm = nn.LSTM(cnn_out_channels, lstm_hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(lstm_hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        with torch.set_grad_enabled(self.distilbert.training):\n",
    "            distilbert_output = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            embeddings = distilbert_output.last_hidden_state.permute(0, 2, 1)  # (batch, embed_dim, seq_len)\n",
    "        \n",
    "        cnn_out = self.cnn(embeddings)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(cnn_out.permute(0, 2, 1))  # (batch, seq_len, lstm_hidden_dim)\n",
    "        \n",
    "        logits = self.fc(lstm_out[:, -1, :])  # Use last hidden state for classification\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNLSTMClassifier(distilbert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vishal Perumal\\anaconda3\\envs\\gpu\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5) #learing rate used by baseline from COLING 2025\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "model.to(device)\n",
    "\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=1):\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batches in tqdm(dataloader):\n",
    "            input_ids = batches['input_ids'].to(device)\n",
    "            attention_mask = batches['attention_mask'].to(device)\n",
    "            labels = batches['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, labels in dataloader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            \n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    print(f'Validation Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4090 [00:00<?, ?it/s]c:\\Users\\Vishal Perumal\\anaconda3\\envs\\gpu\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:403: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "  0%|          | 13/4090 [03:54<22:14:51, 19.64s/it]"
     ]
    }
   ],
   "source": [
    "DB_model = train_model(model, train_dataloader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(DB_model, 'intermediates/DB_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, dev_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
